# Img_classification_Ganesha_Buddha
Image classifier built upon the VGG16 model design and trained weights (modified for binary classification). 

Fot the purpose of a classifying a Ganesha portrait from a Buddha portrait, first I needed to go out and download a bunch of images to of both classes. Thankfully, there exists a way to download images without even opening your favorite browser. The Selenium module comes with processes and functions to automatically open web browser, search for the query you pass into it and a bunch of other functionality to open and access urls. It was incredibly helpful in creating my dataset. I downloaded 400 images for each class. However, a lot of these images failed to download due to broken URL's. This is the part you have to use your own judgement, removing images which are not useful or relevant. Luckily, even after these removals, I was left with about 360 images of each class, which I thought was good enough given I was going to use Keras's Data Augmentation methods for generating my training batches. I kept 300 images of each class in the training sets, and 60 each for validation set. Within both train and validation set, you would want to have sub-directories of Buddha and Ganesha paintings. This helps the train and test generator in Keras to automatically detect class labels. 

Having created the dataset and setting up a smart directory structure, we begin importing and training our classification model. I used VGG16 model with its weights. I modeified the final layer to suit out needs of binary classification as the keras's version of the model has been trained on Imagenet dataset with a 1000 class labels. After altering the structure of the VGG16 network. We start training the final layer of the model, as the rest of structure weights are frozen. Its up to us to set the number of epochs for which we want to train the model, and the batch sizes of images to pass to the model at one time. 10 epochs and a batch size of 10-20 is always a good place to start. I also use history method to remember the accuracy and loss on both traning and validation sets during training. The history method is quite useful as we can access the aforementioned parameters and plot their improvement across the training epochs. This visualization can help us decide on the number of training epochs later.

After implementing the model training code, the accuracy after 10 epochs staibilized aroung 83% on the validation set. For the purpose of seeing how it was doing on some of the images I had, I tested the model predictions against the image itself. The model misclassified one image out of the six I tested. This is not to say the model is doing a great job, but it is doing alright given that the images it trained on were paintings with a lot of artistic variations within each of them.
